#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Created on Sat Nov 11 15:48:53 2017

@author: DataHubBocconi
"""
import stop_words,re, nltk, glob, os
import pandas as pd
    

# This function only wants as input the folder where all your txt files are stored.
# It then joins them all together into a unique txt file called "the great book.txt"
def create_corpus(directory_name):
    '''Puts all the .txt. files in a given folder all together.
    
    Input: directory name
    Output: None. Creates a .txt file called 'the_great_book.txt' in the same
    directory specified in input, containing all the content 
    of the other files separated by a spaces
    '''
    path = os.getcwd()
    os.chdir(directory_name)
    list_of_files = glob.glob('*.txt')
    unique_text = []
    for title in list_of_files:
        print(title)
        with open(title, encoding = 'utf-8') as reader:
            text = reader.read()
        unique_text.append(text)
    with open('the_great_book.txt', 'w', encoding ='utf-8') as writer:
        writer.write(" ".join(unique_text))
    os.chdir(path)
    return

# This function is the one you need to run to clean and analyze the frequency distribution
# of a book. Ideally, you want to pass as book, "the great book" generated by the previous
# function. As directory name, you can just put the directory where such book is stored.
# The function will create two different csv files containing the results of the
# frequency distribution analysis.
def main(directory_name, book = 'the_great_book'):
    print('Starting analysis')
    os.chdir(directory_name)
    print('Set Directory')
    stemmed = clean_book(book)
    print('The text has been cleaned')
    couple = couples(stemmed)
    print('All the couples have been found.')
    freq_dist_couple = freq_dist_couples(couple)
    print('The frequency distribution of the couples has been calculated.')
    dataframe_couples(freq_dist_couple, book+'_couples_distribution')
    print('First DataFrame: done.')
    triplet = triplets(stemmed)
    print('All the triplets have been found')
    freq_dist_triplet = freq_dist_triplets(triplet)
    print('The frequency distribution of the triplets has been calculated.')
    dataframe_triplets(freq_dist_triplet, book+'_triplets_distribution')
    print('Second DataFrame: done.')
    return


def clean_book(book, min_length=4):
    '''Removes stopwords and unuseful characters from a string.
    
    Takes in input the text as string and an optional parameter
    min_length, the length of all the words in outpt will be at 
    least min_length. The default is 4.
    
    Outputs a list of words.
    '''
    os.chdir("d:\\guglielmo documenti\\progetti   programmazione+\\Python\\Leo_immigraz\\Natural-Language-Analysis\\source")
    with open(str(book)+'.txt',encoding='utf-8') as reader:
        text = reader.read()
    #N.B. the order of the following operations is important!
    #remove header
    #text = re.sub(r'.{0,133}', '', text, 1, re.S)
    #remove links (must be done before symbol rem.)
    text = re.sub(r'http\S+', '', text)
    #remove genitivo sassone (must be done before symbol rem.)
    text = re.sub(r'\'s','',text)
    #handle new-line that separates whole words (like considered
    #might become con- \nsidered 
    text = re.sub(r'- \n','', text) 
    #remove symbols
    text = re.sub(r"[^a-zA-Z0-9_\s'-]", '', text)
    #remove stand-alone words (made of any character
    #of length from one to min_length: {1,min_length}
    text = re.sub(r'(?<=\s).{1,'+str(min_length-1)+r'}(?=\s)', '', text)
    #remove new lines
    text = re.sub(r'\n',' ',text)
    text = text.lower()
    # Removes all the stopwords (words which are not characterizing for the meaning of the text)
    for word in stop_words.get_stop_words('english'):
        if len(word) >= min_length:
            text = re.sub(' '+word+' ',' ',text)
    # Removes all the unnecessary spaces.
    text = re.sub(r' {2,}',' ',text)
    words = text.split(' ')
    words.pop()
    # Stems the words
    stemmed = []
    errors = []
    for word in words:
        try:
            stemmed.append(nltk.PorterStemmer().stem(word))
        except IndexError:
            errors.append(word)
    if len(errors) != 0:
        print('Warning: errors found during stemming. Follows list of not-working words.')
        print(errors)		
    return words
 

# couples() returns all the combinations of two consecutives words  
def couples(stemmed):
    return list(nltk.bigrams(stemmed))


def freq_dist_couples(couples):
    return dict(nltk.FreqDist(couples).most_common(300))

# This function creates a DataFrame from the distribution of couples of words and converts it into a CSV.
def dataframe_couples(couples_dist, name):
    df = pd.DataFrame.from_dict(couples_dist, orient = 'index')
    df.to_csv(name+'.csv')
    return df

# triplets() returns all the combinations of three consecutives words.
def triplets(stemmed):
    triplets = []
    for i in range(len(stemmed)-2):
        word1 = stemmed[i]
        word2 = stemmed[i+1]
        word3 = stemmed[i+2]
        triplets.append((word1,word2,word3))
    return triplets

def freq_dist_triplets(triplets):
    return dict(nltk.FreqDist(triplets).most_common(300))

# This function creates a DataFrame from the distribution of triplets of words and converts it into a CSV.
def dataframe_triplets(triplets_dist, name):
    df = pd.DataFrame.from_dict(triplets_dist, orient = 'index')
    df.to_csv(name+'.csv')
    return df

###############################################################################
# IMPLEMENTATION OF GENTZKOW-SHAPIRO, HUGE WIP
###############################################################################

def chi2(v1, v2, nv1, nv2):
    #n = ((v2*nv1-v1*nv2)**2)
    d1 = (v2+v1)*(v2+nv2)
    d2 = (v1+nv1)*(nv1+nv2)
    #return n/(d1 * d2)
    n = ((v2*nv1) - (v1*nv2))
    p1 = n/d1
    p2 = n/d2
    return p1*p2
    

def pearson(c1, c2):
    l = {}
    csv1 = pd.read_csv(c1)
    csv2 = pd.read_csv(c2)
    csv1.columns = ['gram', 'freq']
    csv2.columns = ['gram', 'freq']
    tot1 = sum(csv1['freq'])
    tot2 = sum(csv2['freq'])
    for index, row in csv1.iterrows():
        gram = csv1.loc[index]['gram']
        values1 = csv1.loc[index]['freq']
        if not csv2.loc[csv2['gram'] == gram].empty:
            values2 = int(csv2.loc[csv2['gram'] == gram]['freq'])
        else:
            values2 = 0
        not1 = tot1 - values1
        not2 = tot2 - values2
        stat = chi2(values1, values2, not1, not2)
        l[gram] = stat
    df = pd.DataFrame.from_dict(l, orient = 'index')
    df.to_csv(c1.rstrip('.csv') + '_pearson.csv')
    return l
