#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Created on Sat Nov 11 15:48:53 2017

@author: DataHubBocconi
"""
import stop_words,re, nltk, glob, os
import pandas as pd
    

# This function only wants as input the folder where all your txt files are stored.
# It then joins them all together into a unique txt file called "the great book.txt"
def create_corpus(directory_name):
    os.chdir(directory_name)
    list_of_files = glob.glob('*.txt')
    unique_text = ''
    for title in list_of_files:
        print(title)
        book = open(title, 'r', encoding = 'utf-8', errors='ignore')
        text = book.read()
        unique_text += text
        unique_text += ' '
    the_great_book = open('the_great_book.txt', 'w', encoding ='utf-8', errors='ignore')
    the_great_book.write(unique_text)
    the_great_book.close()
    return

# This function is the one you need to run to clean and analyze the frequency distribution
# of a book. Ideally, you want to pass as book, "the great book" generated by the previous
# function. As directory name, you can just put the directory where such book is stored.
# The function will create two different csv files containing the results of the
# frequency distribution analysis.
def main(directory_name, book = 'the_great_book'):
    print('Starting analysis')
    os.chdir(directory_name)
    print('Set Directory')
    stemmed = clean_book(book)
    print('The text has been cleaned')
    couple = couples(stemmed)
    print('All the couples have been found.')
    freq_dist_couple = freq_dist_couples(couple)
    print('The frequency distribution of the couples has been calculated.')
    dataframe_couples(freq_dist_couple, book+'_couples_distribution')
    print('First DataFrame: done.')
    triplet = triplets(stemmed)
    print('All the triplets have been found')
    freq_dist_triplet = freq_dist_triplets(triplet)
    print('The frequency distribution of the triplets has been calculated.')
    dataframe_triplets(freq_dist_triplet, book+'_triplets_distribution')
    print('Second DataFrame: done.')
    return


def clean_book(book):
    with open(str(book)+'.txt',encoding='utf-8') as reader:
	    text = reader.read()
    # Remove all the characters which are used by Python to go to a new line
    text = re.sub(r'- \n','', text)
    text = re.sub(r'\n',' ',text)
    print('New lines removed')
    # Here we start cleaning the text, we will tokenize it at the end to make the process 
    # more efficient
    # Remove the genitivo sassone
    text = re.sub(r'\'s','',text)
    # Remove everything which is not a letter, thus punctuation and numbers. 
    # Then it lower-cases everything
    text = re.sub(r'[^a-zA-z ]', '',  text).lower()
    # Removes all the unnecessary spaces.
    text = re.sub(r' {2,}',' ',text)
    text = re.sub(r' [a-zA-z] ', ' ', text)
    print('Non-letters removed')
    #f = re.sub(r'( a | about | above | after | again | against | all | am | an | and | any | are | aren\'t | as | at | be | because | been | before | being | below | between | both | but | by | can\'t | cannot | could | couldn\'t | did | didn\'t | do | does | doesn\'t | doing | don\'t | down | during | each | few | for | from | further | had | hadn\'t | has | hasn\'t | have | haven\'t | having | he | he\'d | he\'ll | he\'s | her | here | here\'s | hers | herself | him | himself | his | how | how\'s | i | i\'d | i\'ll | i\'m | i\'ve | if | in | into | is | isn\'t | it | it\'s | its | itself | let\'s | me | more | most | mustn\'t | my | myself | no | nor | not | of | off | on | once | only | or | other | ought | our | ours | ourselves | out | over | own | same | shan\'t | she | she\'d | she\'ll | she\'s | should | shouldn\'t | so | some | such | than | that | that\'s | the | their | theirs | them | themselves | then | there | there\'s | these | they | they\'d | they\'ll | they\'re | they\'ve | this | those | through | to | too | under | until | up | very | was | wasn\'t | we | we\'d | we\'ll | we\'re | we\'ve | were | weren\'t | what | what\'s | when | when\'s | where | where\'s | which | while | who | who\'s | whom | why | why\'s | with | won\'t | would | wouldn\'t | you | you\'d | you\'ll | you\'re | you\'ve | your | yours | yourself | yourselves )', ' ', e)
    # Removes all the stopwords (words which are not characterizing for the meaning of the text)
    for i in stop_words.get_stop_words('english'):
        text = re.sub(' '+i+' ',' ',text)
    print('Stop-words removed')
    # Tokenize the clean text
    words = text.split(' ')
    print('Tokenized')
    print('There are ',len(words), ' words')
    # Stems the words
    stemmed = [nltk.PorterStemmer().stem(t) for t in words]
    print('Stemmed')
    return stemmed
 

# couples() returns all the combinations of two consecutives words  
def couples(stemmed):
    return list(nltk.bigrams(stemmed))


def freq_dist_couples(couples):
    return dict(nltk.FreqDist(couples).most_common(300))

# This function creates a DataFrame from the distribution of couples of words and converts it into a CSV.
def dataframe_couples(couples_dist, name):
    df = pd.DataFrame.from_dict(couples_dist, orient = 'index')
    df.to_csv(name+'.csv')
    return df

# triplets() returns all the combinations of three consecutives words.
def triplets(stemmed):
    triplets = []
    for i in range(len(stemmed)-2):
        word1 = stemmed[i]
        word2 = stemmed[i+1]
        word3 = stemmed[i+2]
        triplets.append((word1,word2,word3))
    return triplets

def freq_dist_triplets(triplets):
    return dict(nltk.FreqDist(triplets).most_common(300))

# This function creates a DataFrame from the distribution of triplets of words and converts it into a CSV.
def dataframe_triplets(triplets_dist, name):
    df = pd.DataFrame.from_dict(triplets_dist, orient = 'index')
    df.to_csv(name+'.csv')
    return df

###############################################################################
# IMPLEMENTATION OF GENTZKOW-SHAPIRO, HUGE WIP
###############################################################################

def chi2(v1, v2, nv1, nv2):
    #n = ((v2*nv1-v1*nv2)**2)
    d1 = (v2+v1)*(v2+nv2)
    d2 = (v1+nv1)*(nv1+nv2)
    #return n/(d1 * d2)
    n = ((v2*nv1) - (v1*nv2))
    p1 = n/d1
    p2 = n/d2
    return p1*p2
    

def pearson(c1, c2):
    l = {}
    csv1 = pd.read_csv(c1)
    csv2 = pd.read_csv(c2)
    csv1.columns = ['gram', 'freq']
    csv2.columns = ['gram', 'freq']
    tot1 = sum(csv1['freq'])
    tot2 = sum(csv2['freq'])
    for index, row in csv1.iterrows():
        gram = csv1.loc[index]['gram']
        values1 = csv1.loc[index]['freq']
        if not csv2.loc[csv2['gram'] == gram].empty:
            values2 = int(csv2.loc[csv2['gram'] == gram]['freq'])
        else:
            values2 = 0
        not1 = tot1 - values1
        not2 = tot2 - values2
        stat = chi2(values1, values2, not1, not2)
        l[gram] = stat
    df = pd.DataFrame.from_dict(l, orient = 'index')
    df.to_csv(c1.rstrip('.csv') + '_pearson.csv')
    return l
